{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ishiki Detector\n",
    "君の意識は高いか。\n",
    "文章の意識高さを判定するニューラルネットワークモデル。\n",
    "\n",
    "## Setup\n",
    "- jupyter でこの ipynb を立ち上げてください\n",
    "- 適当な場所に以下の2つのテキストファイルを置いてください（改行区切り）\n",
    "    - 意識高い文を20万個くらい集めたテキストファイル\n",
    "    - 意識低い文を20万個くらい集めたテキストファイル\n",
    "- 私は前者は [TED](http://logmi.jp/tag/ted) の文章、後者は Twitter の文章を使いました。\n",
    "\n",
    "## Run\n",
    "### Training\n",
    "上から順に Train まで実行してください。\n",
    "\n",
    "- pos_path, neg_path は Setup で集めたデータのパスに書き換えてください。\n",
    "- save_dir はモデルを保存するディレクトリパスに書き換えてください。\n",
    "\n",
    "### Restore & Predict\n",
    "Preprocess Data 及び、 Restore 以降を実行してください。\n",
    "\n",
    "- prediction_text_list は判定をしたい文章を改行区切りで入力してください\n",
    "- uses_sort を True にすれば結果が意識低い順にソートされます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = '/home/harumitsu.nobuta/tmp/ishiki_detector/ishikitakai.txt'\n",
    "neg_path = '/home/harumitsu.nobuta/tmp/ishiki_detector/ishikihikui.txt'\n",
    "validation_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id_dic = {'': 0}  # 文字 -> ID の変換辞書\n",
    "id2char_dic = {0: ''}  # ID -> 文字の変換辞書\n",
    "\n",
    "def text2id_list(text):\n",
    "    return [char2id_dic[c] for c in text]\n",
    "\n",
    "def id_list2text(id_list):\n",
    "    return ''.join([id2char_dic[id_] for id_ in id_list])\n",
    "\n",
    "def update_char_dict(c):\n",
    "    if c not in char2id_dic:\n",
    "        new_id = len(char2id_dic)\n",
    "        char2id_dic[c] = new_id\n",
    "        id2char_dic[new_id] = c\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path) as f:\n",
    "        raw_text = f.read()\n",
    "    for c in raw_text:\n",
    "        update_char_dict(c)\n",
    "    text_list = [line.strip() for line in raw_text.split()]\n",
    "    text_list = list(set(text_list))  # 重複を削除\n",
    "    id_list_list = [text2id_list(text) for text in text_list]\n",
    "    return id_list_list\n",
    "\n",
    "def convert_padded_array(id_list_list, max_length):\n",
    "    return np.array([[0] * (max_length - len(id_list)) + id_list for id_list in id_list_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 200000\n",
      "neg: 221309\n",
      "result data num: 200000\n",
      "max_length: 85\n"
     ]
    }
   ],
   "source": [
    "pos_data = load_data(pos_path)\n",
    "neg_data = load_data(neg_path)\n",
    "print('pos: {}\\nneg: {}'.format(len(pos_data), len(neg_data)))\n",
    "\n",
    "# pos/neg の数を同じにする\n",
    "data_num = min(len(pos_data), len(neg_data))\n",
    "print('result data num: {}'.format(data_num))\n",
    "pos_data = pos_data[:data_num]\n",
    "neg_data = neg_data[:data_num]\n",
    "\n",
    "max_length = max([len(id_list) for id_list in pos_data + neg_data])\n",
    "print('max_length: {}'.format(max_length))\n",
    "\n",
    "half_validation_num = int(validation_num / 2)\n",
    "validation_x = convert_padded_array(pos_data[:half_validation_num] + neg_data[:half_validation_num], max_length)\n",
    "validation_y = np.array([1] * half_validation_num + [0] * half_validation_num)\n",
    "train_x = convert_padded_array(pos_data[half_validation_num:] + neg_data[half_validation_num:], max_length)\n",
    "train_y = np.array([1] * (data_num - half_validation_num) + [0] * (data_num - half_validation_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "rnn_hidden_dim = 512\n",
    "dense_layer_num = 2\n",
    "dense_hidden_dim = 128\n",
    "dropout = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/harumitsu.nobuta/.pyenv/versions/3.5.2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/harumitsu.nobuta/.pyenv/versions/3.5.2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "vocab_size = len(char2id_dic)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(GRU(rnn_hidden_dim, return_sequences=False, dropout=dropout))\n",
    "for l in range(dense_layer_num):\n",
    "    model.add(Dense(dense_hidden_dim))\n",
    "    model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/harumitsu.nobuta/tmp/ishiki_detector/log/base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "epoch_num = 2\n",
    "early_stopping_patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, 'model.json'), 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_num,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=early_stopping_patience),\n",
    "        TensorBoard(os.path.join(save_dir, 'log')),\n",
    "    ],\n",
    "    shuffle=True,\n",
    ")\n",
    "model.save_weights(os.path.join(save_dir, 'param.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "with open(os.path.join(save_dir, 'model.json')) as f:\n",
    "    model = model_from_json(f.read())\n",
    "model.load_weights(os.path.join(save_dir, 'param.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "意識高さ  100.0% - 😎ミクシィグループは『新しい文化を創る』をミッションに掲げコミュニケーションを軸にした新しい価値の提供により新たな市場の創造に挑戦する企業グループです\n",
      "意識高さ   83.4% - 😎データサイエンティストの宿命\n",
      "意識高さ   99.9% - 😎世界の平和を保つため、我々は行動しなくてはならないのです\n",
      "意識高さ   94.7% - 😎高負荷耐性な設計ができるエンジニアが求められている\n",
      "意識高さ   79.2% - 😎我々もやらねばならぬのです\n",
      "意識高さ  100.0% - 😎------------------------------\n",
      "意識高さ   57.2% - 😎圧倒的当事者意識\n",
      "意識高さ   27.1% - 😜学習率のDecayがちょっと遅くなっていると思いますがどのOptimizerを使っていますか\n",
      "意識高さ    2.1% - 😜学習率のDecayがちょっと遅いけどOptimizerを使ってるん\n",
      "意識高さ  100.0% - 😎------------------------------\n",
      "意識高さ    2.6% - 😜ビール飲みたい\n",
      "意識高さ   10.5% - 😜業務ほったらかして意識高い判定器作ってるんだけどこれ意識低くない\n",
      "意識高さ    0.3% - 😜お腹へってるときに飯テロされると辛い\n",
      "意識高さ   43.1% - 😜どうでもええからさっさと学習終われや\n",
      "意識高さ    1.5% - 😜誕生日おめでとー\n",
      "意識高さ    5.4% - 😜まじありえない\n",
      "意識高さ    0.5% - 😜オカメインコとコザクラインコ、どっちがかわいいかまよううううううう\n",
      "意識高さ   12.9% - 😜働きたくないでござる\n"
     ]
    }
   ],
   "source": [
    "prediction_text_list = '''\n",
    "ミクシィグループは『新しい文化を創る』をミッションに掲げコミュニケーションを軸にした新しい価値の提供により新たな市場の創造に挑戦する企業グループです\n",
    "データサイエンティストの宿命\n",
    "世界の平和を保つため、我々は行動しなくてはならないのです\n",
    "高負荷耐性な設計ができるエンジニアが求められている\n",
    "我々もやらねばならぬのです\n",
    "------------------------------\n",
    "圧倒的当事者意識\n",
    "学習率のDecayがちょっと遅くなっていると思いますがどのOptimizerを使っていますか\n",
    "学習率のDecayがちょっと遅いけどOptimizerを使ってるん\n",
    "------------------------------\n",
    "ビール飲みたい\n",
    "業務ほったらかして意識高い判定器作ってるんだけどこれ意識低くない\n",
    "お腹へってるときに飯テロされると辛い\n",
    "どうでもええからさっさと学習終われや\n",
    "誕生日おめでとー\n",
    "まじありえない\n",
    "オカメインコとコザクラインコ、どっちがかわいいかまよううううううう\n",
    "働きたくないでござる\n",
    "'''.split()\n",
    "uses_sort = False\n",
    "\n",
    "pred_id_list_list = [text2id_list(text) for text in prediction_text_list]\n",
    "pred_x = convert_padded_array(pred_id_list_list, max_length)\n",
    "result_list = model.predict(pred_x)\n",
    "\n",
    "pairs = list(zip(prediction_text_list, result_list))\n",
    "if uses_sort:\n",
    "    pairs.sort(key=lambda x: x[1][0])\n",
    "for text, result in pairs:\n",
    "    pos_rate = result[0]\n",
    "    prefix = '😎' if pos_rate > 0.5 else '😜'\n",
    "    print('意識高さ {: 6.1f}% - {}'.format(pos_rate * 100, prefix + text))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
