{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ishiki Detector\n",
    "Âêõ„ÅÆÊÑèË≠ò„ÅØÈ´ò„ÅÑ„Åã„ÄÇ\n",
    "ÊñáÁ´†„ÅÆÊÑèË≠òÈ´ò„Åï„ÇíÂà§ÂÆö„Åô„Çã„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„É¢„Éá„É´„ÄÇ\n",
    "\n",
    "## Setup\n",
    "- jupyter „Åß„Åì„ÅÆ ipynb „ÇíÁ´ã„Å°‰∏ä„Åí„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "- ÈÅ©ÂΩì„Å™Â†¥ÊâÄ„Å´‰ª•‰∏ã„ÅÆ2„Å§„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´„ÇíÁΩÆ„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑÔºàÊîπË°åÂå∫Âàá„ÇäÔºâ\n",
    "    - ÊÑèË≠òÈ´ò„ÅÑÊñá„Çí20‰∏áÂÄã„Åè„Çâ„ÅÑÈõÜ„ÇÅ„Åü„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´\n",
    "    - ÊÑèË≠ò‰Ωé„ÅÑÊñá„Çí20‰∏áÂÄã„Åè„Çâ„ÅÑÈõÜ„ÇÅ„Åü„ÉÜ„Ç≠„Çπ„Éà„Éï„Ç°„Ç§„É´\n",
    "- ÁßÅ„ÅØÂâçËÄÖ„ÅØ [TED](http://logmi.jp/tag/ted) „ÅÆÊñáÁ´†„ÄÅÂæåËÄÖ„ÅØ Twitter „ÅÆÊñáÁ´†„Çí‰Ωø„ÅÑ„Åæ„Åó„Åü„ÄÇ\n",
    "\n",
    "## Run\n",
    "### Training\n",
    "‰∏ä„Åã„ÇâÈ†Ü„Å´ Train „Åæ„ÅßÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "- pos_path, neg_path „ÅØ Setup „ÅßÈõÜ„ÇÅ„Åü„Éá„Éº„Çø„ÅÆ„Éë„Çπ„Å´Êõ∏„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "- save_dir „ÅØ„É¢„Éá„É´„Çí‰øùÂ≠ò„Åô„Çã„Éá„Ç£„É¨„ÇØ„Éà„É™„Éë„Çπ„Å´Êõ∏„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "### Restore & Predict\n",
    "Preprocess Data Âèä„Å≥„ÄÅ Restore ‰ª•Èôç„ÇíÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "\n",
    "- prediction_text_list „ÅØÂà§ÂÆö„Çí„Åó„Åü„ÅÑÊñáÁ´†„ÇíÊîπË°åÂå∫Âàá„Çä„ÅßÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
    "- uses_sort „Çí True „Å´„Åô„Çå„Å∞ÁµêÊûú„ÅåÊÑèË≠ò‰Ωé„ÅÑÈ†Ü„Å´„ÇΩ„Éº„Éà„Åï„Çå„Åæ„Åô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = '/home/harumitsu.nobuta/tmp/ishiki_detector/ishikitakai.txt'\n",
    "neg_path = '/home/harumitsu.nobuta/tmp/ishiki_detector/ishikihikui.txt'\n",
    "validation_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id_dic = {'': 0}  # ÊñáÂ≠ó -> ID „ÅÆÂ§âÊèõËæûÊõ∏\n",
    "id2char_dic = {0: ''}  # ID -> ÊñáÂ≠ó„ÅÆÂ§âÊèõËæûÊõ∏\n",
    "\n",
    "def text2id_list(text):\n",
    "    return [char2id_dic[c] for c in text]\n",
    "\n",
    "def id_list2text(id_list):\n",
    "    return ''.join([id2char_dic[id_] for id_ in id_list])\n",
    "\n",
    "def update_char_dict(c):\n",
    "    if c not in char2id_dic:\n",
    "        new_id = len(char2id_dic)\n",
    "        char2id_dic[c] = new_id\n",
    "        id2char_dic[new_id] = c\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path) as f:\n",
    "        raw_text = f.read()\n",
    "    for c in raw_text:\n",
    "        update_char_dict(c)\n",
    "    text_list = [line.strip() for line in raw_text.split()]\n",
    "    text_list = list(set(text_list))  # ÈáçË§á„ÇíÂâäÈô§\n",
    "    id_list_list = [text2id_list(text) for text in text_list]\n",
    "    return id_list_list\n",
    "\n",
    "def convert_padded_array(id_list_list, max_length):\n",
    "    return np.array([[0] * (max_length - len(id_list)) + id_list for id_list in id_list_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 200000\n",
      "neg: 221309\n",
      "result data num: 200000\n",
      "max_length: 85\n"
     ]
    }
   ],
   "source": [
    "pos_data = load_data(pos_path)\n",
    "neg_data = load_data(neg_path)\n",
    "print('pos: {}\\nneg: {}'.format(len(pos_data), len(neg_data)))\n",
    "\n",
    "# pos/neg „ÅÆÊï∞„ÇíÂêå„Åò„Å´„Åô„Çã\n",
    "data_num = min(len(pos_data), len(neg_data))\n",
    "print('result data num: {}'.format(data_num))\n",
    "pos_data = pos_data[:data_num]\n",
    "neg_data = neg_data[:data_num]\n",
    "\n",
    "max_length = max([len(id_list) for id_list in pos_data + neg_data])\n",
    "print('max_length: {}'.format(max_length))\n",
    "\n",
    "half_validation_num = int(validation_num / 2)\n",
    "validation_x = convert_padded_array(pos_data[:half_validation_num] + neg_data[:half_validation_num], max_length)\n",
    "validation_y = np.array([1] * half_validation_num + [0] * half_validation_num)\n",
    "train_x = convert_padded_array(pos_data[half_validation_num:] + neg_data[half_validation_num:], max_length)\n",
    "train_y = np.array([1] * (data_num - half_validation_num) + [0] * (data_num - half_validation_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "rnn_hidden_dim = 512\n",
    "dense_layer_num = 2\n",
    "dense_hidden_dim = 128\n",
    "dropout = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/harumitsu.nobuta/.pyenv/versions/3.5.2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1192: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/harumitsu.nobuta/.pyenv/versions/3.5.2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:1299: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "vocab_size = len(char2id_dic)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "model.add(GRU(rnn_hidden_dim, return_sequences=False, dropout=dropout))\n",
    "for l in range(dense_layer_num):\n",
    "    model.add(Dense(dense_hidden_dim))\n",
    "    model.add(Activation(\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/harumitsu.nobuta/tmp/ishiki_detector/log/base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "epoch_num = 2\n",
    "early_stopping_patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "with open(os.path.join(save_dir, 'model.json'), 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoch_num,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=early_stopping_patience),\n",
    "        TensorBoard(os.path.join(save_dir, 'log')),\n",
    "    ],\n",
    "    shuffle=True,\n",
    ")\n",
    "model.save_weights(os.path.join(save_dir, 'param.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import json\n",
    "with open(os.path.join(save_dir, 'model.json')) as f:\n",
    "    model = model_from_json(f.read())\n",
    "model.load_weights(os.path.join(save_dir, 'param.hdf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊÑèË≠òÈ´ò„Åï  100.0% - üòé„Éü„ÇØ„Ç∑„Ç£„Ç∞„É´„Éº„Éó„ÅØ„ÄéÊñ∞„Åó„ÅÑÊñáÂåñ„ÇíÂâµ„Çã„Äè„Çí„Éü„ÉÉ„Ç∑„Éß„É≥„Å´Êé≤„Åí„Ç≥„Éü„É•„Éã„Ç±„Éº„Ç∑„Éß„É≥„ÇíËª∏„Å´„Åó„ÅüÊñ∞„Åó„ÅÑ‰æ°ÂÄ§„ÅÆÊèê‰æõ„Å´„Çà„ÇäÊñ∞„Åü„Å™Â∏ÇÂ†¥„ÅÆÂâµÈÄ†„Å´ÊåëÊà¶„Åô„Çã‰ºÅÊ•≠„Ç∞„É´„Éº„Éó„Åß„Åô\n",
      "ÊÑèË≠òÈ´ò„Åï   83.4% - üòé„Éá„Éº„Çø„Çµ„Ç§„Ç®„É≥„ÉÜ„Ç£„Çπ„Éà„ÅÆÂÆøÂëΩ\n",
      "ÊÑèË≠òÈ´ò„Åï   99.9% - üòé‰∏ñÁïå„ÅÆÂπ≥Âíå„Çí‰øù„Å§„Åü„ÇÅ„ÄÅÊàë„ÄÖ„ÅØË°åÂãï„Åó„Å™„Åè„Å¶„ÅØ„Å™„Çâ„Å™„ÅÑ„ÅÆ„Åß„Åô\n",
      "ÊÑèË≠òÈ´ò„Åï   94.7% - üòéÈ´òË≤†Ëç∑ËÄêÊÄß„Å™Ë®≠Ë®à„Åå„Åß„Åç„Çã„Ç®„É≥„Ç∏„Éã„Ç¢„ÅåÊ±Ç„ÇÅ„Çâ„Çå„Å¶„ÅÑ„Çã\n",
      "ÊÑèË≠òÈ´ò„Åï   79.2% - üòéÊàë„ÄÖ„ÇÇ„ÇÑ„Çâ„Å≠„Å∞„Å™„Çâ„Å¨„ÅÆ„Åß„Åô\n",
      "ÊÑèË≠òÈ´ò„Åï  100.0% - üòé------------------------------\n",
      "ÊÑèË≠òÈ´ò„Åï   57.2% - üòéÂúßÂÄíÁöÑÂΩì‰∫ãËÄÖÊÑèË≠ò\n",
      "ÊÑèË≠òÈ´ò„Åï   27.1% - üòúÂ≠¶ÁøíÁéá„ÅÆDecay„Åå„Å°„Çá„Å£„Å®ÈÅÖ„Åè„Å™„Å£„Å¶„ÅÑ„Çã„Å®ÊÄù„ÅÑ„Åæ„Åô„Åå„Å©„ÅÆOptimizer„Çí‰Ωø„Å£„Å¶„ÅÑ„Åæ„Åô„Åã\n",
      "ÊÑèË≠òÈ´ò„Åï    2.1% - üòúÂ≠¶ÁøíÁéá„ÅÆDecay„Åå„Å°„Çá„Å£„Å®ÈÅÖ„ÅÑ„Åë„Å©Optimizer„Çí‰Ωø„Å£„Å¶„Çã„Çì\n",
      "ÊÑèË≠òÈ´ò„Åï  100.0% - üòé------------------------------\n",
      "ÊÑèË≠òÈ´ò„Åï    2.6% - üòú„Éì„Éº„É´È£≤„Åø„Åü„ÅÑ\n",
      "ÊÑèË≠òÈ´ò„Åï   10.5% - üòúÊ•≠Âãô„Åª„Å£„Åü„Çâ„Åã„Åó„Å¶ÊÑèË≠òÈ´ò„ÅÑÂà§ÂÆöÂô®‰Ωú„Å£„Å¶„Çã„Çì„Å†„Åë„Å©„Åì„ÇåÊÑèË≠ò‰Ωé„Åè„Å™„ÅÑ\n",
      "ÊÑèË≠òÈ´ò„Åï    0.3% - üòú„ÅäËÖπ„Å∏„Å£„Å¶„Çã„Å®„Åç„Å´È£Ø„ÉÜ„É≠„Åï„Çå„Çã„Å®Ëæõ„ÅÑ\n",
      "ÊÑèË≠òÈ´ò„Åï   43.1% - üòú„Å©„ÅÜ„Åß„ÇÇ„Åà„Åà„Åã„Çâ„Åï„Å£„Åï„Å®Â≠¶ÁøíÁµÇ„Çè„Çå„ÇÑ\n",
      "ÊÑèË≠òÈ´ò„Åï    1.5% - üòúË™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„Éº\n",
      "ÊÑèË≠òÈ´ò„Åï    5.4% - üòú„Åæ„Åò„ÅÇ„Çä„Åà„Å™„ÅÑ\n",
      "ÊÑèË≠òÈ´ò„Åï    0.5% - üòú„Ç™„Ç´„É°„Ç§„É≥„Ç≥„Å®„Ç≥„Ç∂„ÇØ„É©„Ç§„É≥„Ç≥„ÄÅ„Å©„Å£„Å°„Åå„Åã„Çè„ÅÑ„ÅÑ„Åã„Åæ„Çà„ÅÜ„ÅÜ„ÅÜ„ÅÜ„ÅÜ„ÅÜ„ÅÜ\n",
      "ÊÑèË≠òÈ´ò„Åï   12.9% - üòúÂÉç„Åç„Åü„Åè„Å™„ÅÑ„Åß„Åî„Åñ„Çã\n"
     ]
    }
   ],
   "source": [
    "prediction_text_list = '''\n",
    "„Éü„ÇØ„Ç∑„Ç£„Ç∞„É´„Éº„Éó„ÅØ„ÄéÊñ∞„Åó„ÅÑÊñáÂåñ„ÇíÂâµ„Çã„Äè„Çí„Éü„ÉÉ„Ç∑„Éß„É≥„Å´Êé≤„Åí„Ç≥„Éü„É•„Éã„Ç±„Éº„Ç∑„Éß„É≥„ÇíËª∏„Å´„Åó„ÅüÊñ∞„Åó„ÅÑ‰æ°ÂÄ§„ÅÆÊèê‰æõ„Å´„Çà„ÇäÊñ∞„Åü„Å™Â∏ÇÂ†¥„ÅÆÂâµÈÄ†„Å´ÊåëÊà¶„Åô„Çã‰ºÅÊ•≠„Ç∞„É´„Éº„Éó„Åß„Åô\n",
    "„Éá„Éº„Çø„Çµ„Ç§„Ç®„É≥„ÉÜ„Ç£„Çπ„Éà„ÅÆÂÆøÂëΩ\n",
    "‰∏ñÁïå„ÅÆÂπ≥Âíå„Çí‰øù„Å§„Åü„ÇÅ„ÄÅÊàë„ÄÖ„ÅØË°åÂãï„Åó„Å™„Åè„Å¶„ÅØ„Å™„Çâ„Å™„ÅÑ„ÅÆ„Åß„Åô\n",
    "È´òË≤†Ëç∑ËÄêÊÄß„Å™Ë®≠Ë®à„Åå„Åß„Åç„Çã„Ç®„É≥„Ç∏„Éã„Ç¢„ÅåÊ±Ç„ÇÅ„Çâ„Çå„Å¶„ÅÑ„Çã\n",
    "Êàë„ÄÖ„ÇÇ„ÇÑ„Çâ„Å≠„Å∞„Å™„Çâ„Å¨„ÅÆ„Åß„Åô\n",
    "------------------------------\n",
    "ÂúßÂÄíÁöÑÂΩì‰∫ãËÄÖÊÑèË≠ò\n",
    "Â≠¶ÁøíÁéá„ÅÆDecay„Åå„Å°„Çá„Å£„Å®ÈÅÖ„Åè„Å™„Å£„Å¶„ÅÑ„Çã„Å®ÊÄù„ÅÑ„Åæ„Åô„Åå„Å©„ÅÆOptimizer„Çí‰Ωø„Å£„Å¶„ÅÑ„Åæ„Åô„Åã\n",
    "Â≠¶ÁøíÁéá„ÅÆDecay„Åå„Å°„Çá„Å£„Å®ÈÅÖ„ÅÑ„Åë„Å©Optimizer„Çí‰Ωø„Å£„Å¶„Çã„Çì\n",
    "------------------------------\n",
    "„Éì„Éº„É´È£≤„Åø„Åü„ÅÑ\n",
    "Ê•≠Âãô„Åª„Å£„Åü„Çâ„Åã„Åó„Å¶ÊÑèË≠òÈ´ò„ÅÑÂà§ÂÆöÂô®‰Ωú„Å£„Å¶„Çã„Çì„Å†„Åë„Å©„Åì„ÇåÊÑèË≠ò‰Ωé„Åè„Å™„ÅÑ\n",
    "„ÅäËÖπ„Å∏„Å£„Å¶„Çã„Å®„Åç„Å´È£Ø„ÉÜ„É≠„Åï„Çå„Çã„Å®Ëæõ„ÅÑ\n",
    "„Å©„ÅÜ„Åß„ÇÇ„Åà„Åà„Åã„Çâ„Åï„Å£„Åï„Å®Â≠¶ÁøíÁµÇ„Çè„Çå„ÇÑ\n",
    "Ë™ïÁîüÊó•„Åä„ÇÅ„Åß„Å®„Éº\n",
    "„Åæ„Åò„ÅÇ„Çä„Åà„Å™„ÅÑ\n",
    "„Ç™„Ç´„É°„Ç§„É≥„Ç≥„Å®„Ç≥„Ç∂„ÇØ„É©„Ç§„É≥„Ç≥„ÄÅ„Å©„Å£„Å°„Åå„Åã„Çè„ÅÑ„ÅÑ„Åã„Åæ„Çà„ÅÜ„ÅÜ„ÅÜ„ÅÜ„ÅÜ„ÅÜ„ÅÜ\n",
    "ÂÉç„Åç„Åü„Åè„Å™„ÅÑ„Åß„Åî„Åñ„Çã\n",
    "'''.split()\n",
    "uses_sort = False\n",
    "\n",
    "pred_id_list_list = [text2id_list(text) for text in prediction_text_list]\n",
    "pred_x = convert_padded_array(pred_id_list_list, max_length)\n",
    "result_list = model.predict(pred_x)\n",
    "\n",
    "pairs = list(zip(prediction_text_list, result_list))\n",
    "if uses_sort:\n",
    "    pairs.sort(key=lambda x: x[1][0])\n",
    "for text, result in pairs:\n",
    "    pos_rate = result[0]\n",
    "    prefix = 'üòé' if pos_rate > 0.5 else 'üòú'\n",
    "    print('ÊÑèË≠òÈ´ò„Åï {: 6.1f}% - {}'.format(pos_rate * 100, prefix + text))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
